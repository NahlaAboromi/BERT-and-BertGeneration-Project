<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="refresh" content="0; URL='https://nahlaaboromi.github.io/BERT-and-BertGeneration-Project/NEW_ARCH.html'" />
    <title>BERT Transformer Encoder-Decoder Model Architecture</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f8ff;
            margin: 0;
            padding: 20px;
            color: #333;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            padding: 30px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        h1 {
            text-align: center;
            color: #2c5aa0;
            font-size: 1.8em;
            margin-bottom: 40px;
            font-weight: bold;
        }

        .architecture-flow {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
            margin-bottom: 40px;
        }

        .component {
            border: 2px solid;
            border-radius: 8px;
            padding: 15px 25px;
            text-align: center;
            min-width: 160px;
            position: relative;
        }

        .input-box {
            background-color: #e6f3ff;
            border-color: #4a90e2;
            color: #2c5aa0;
        }

        .tokenizer-box {
            background-color: #fff2cc;
            border-color: #d6b656;
            color: #8b7355;
        }

        .embedding-box {
            background-color: #e6ffe6;
            border-color: #4caf50;
            color: #2e7d32;
        }

        .encoder-decoder-container {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            width: 100%;
            max-width: 800px;
            position: relative;
            margin: 20px 0;
        }

        .encoder-box {
            background-color: #f0e6ff;
            border-color: #8e44ad;
            color: #6a1b9a;
            width: 300px;
            min-height: 200px;
        }

        .decoder-box {
            background-color: #ffe6e6;
            border-color: #e74c3c;
            color: #c0392b;
            width: 300px;
            min-height: 200px;
        }

        .attention-detail {
            background-color: #fff8dc;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 8px;
            margin: 8px 0;
            font-size: 0.85em;
            text-align: left;
        }

        .output-box {
            background-color: #e6f7ff;
            border-color: #1890ff;
            color: #0050b3;
        }

        .generated-box {
            background-color: #f6ffed;
            border-color: #52c41a;
            color: #389e0d;
        }

        .component-title {
            font-weight: bold;
            font-size: 1.1em;
            margin-bottom: 5px;
        }

        .component-subtitle {
            font-size: 0.9em;
            margin-bottom: 10px;
            font-style: italic;
        }

        .arrow {
            font-size: 24px;
            color: #666;
            margin: 5px 0;
        }

        .cross-attention-arrow {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background-color: #fff;
            border: 2px dashed #ff6b6b;
            border-radius: 6px;
            padding: 8px 12px;
            font-size: 0.8em;
            color: #d63031;
            z-index: 10;
            white-space: nowrap;
        }

        .specifications-container {
            display: flex;
            justify-content: space-between;
            gap: 30px;
            margin-top: 30px;
        }

        .spec-box {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px;
            flex: 1;
        }

        .spec-title {
            font-weight: bold;
            color: #495057;
            margin-bottom: 15px;
            font-size: 1.1em;
        }

        .spec-list {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .spec-item {
            margin: 8px 0;
            font-size: 0.9em;
            color: #6c757d;
            display: flex;
            align-items: center;
        }

        .spec-item::before {
            content: "•";
            color: #007bff;
            margin-right: 8px;
            font-weight: bold;
        }

        @media (max-width: 768px) {
            .encoder-decoder-container {
                flex-direction: column;
                align-items: center;
                gap: 20px;
            }
            
            .encoder-box, .decoder-box {
                width: 100%;
                max-width: 350px;
            }
            
            .cross-attention-arrow {
                position: static;
                transform: none;
                margin: 15px 0;
            }
            
            .specifications-container {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>BERT and BertGeneration Project:<br>Shakespearean Text Generation<br><span style="font-size: 0.9em; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; font-weight: 600; text-shadow: 0 2px 4px rgba(102, 126, 234, 0.2); letter-spacing: 0.5px;">Modern Transformer Architecture - Revolutionary Seq2Seq Enhancement with Self-Attention</span></h1>
        
        <div class="architecture-flow">
            <!-- Input Text -->
            <div class="component input-box">
                <div class="component-title">Input Text</div>
                <div class="component-subtitle">(Shakespeare)</div>
            </div>

            <div class="arrow">↓</div>

            <!-- Tokenizer -->
            <div class="component tokenizer-box">
                <div class="component-title">Tokenizer</div>
                <div class="component-subtitle">(BERT base-uncased)</div>
            </div>

            <div class="arrow">↓</div>

            <!-- Embedding Layer -->
            <div class="component embedding-box">
                <div class="component-title">Embedding Layer</div>
                <div class="component-subtitle">(Token → Vector)</div>
            </div>

            <div class="arrow">↓</div>

            <!-- Encoder-Decoder Container -->
            <div class="encoder-decoder-container">
                <!-- Encoder -->
                <div class="component encoder-box">
                    <div class="component-title">Encoder Layer</div>
                    <div class="component-subtitle">Multi-Head Self-Attention<br>Multi-Layer Processing<br>(12 Transformer Layers)</div>
                    
                    <div class="attention-detail">
                        <strong>Self-Attention:</strong><br>
                        Parallel processing of entire sequence
                    </div>
                    
                    <div class="attention-detail">
                        <strong>Feed Forward:</strong><br>
                        Position-wise fully connected layers
                    </div>
                    
                    <div class="attention-detail">
                        <strong>Normalization:</strong><br>
                        Layer Norm + Residual Connections
                    </div>
                </div>

                <!-- Cross-Attention -->
                <div class="cross-attention-arrow">
                    Cross-Attention
                </div>

                <!-- Decoder -->
                <div class="component decoder-box">
                    <div class="component-title">Decoder Layer</div>
                    <div class="component-subtitle">Masked Self-Attention<br>Cross-Attention<br>(12 Transformer Layers)</div>
                    
                    <div class="attention-detail">
                        <strong>Masked Self-Attention:</strong><br>
                        Prevents looking ahead in sequence
                    </div>
                    
                    <div class="attention-detail">
                        <strong>Cross-Attention:</strong><br>
                        Attends to encoder outputs
                    </div>
                    
                    <div class="attention-detail">
                        <strong>Feed Forward:</strong><br>
                        Position-wise processing + Norm
                    </div>
                </div>
            </div>

            <div class="arrow">↓</div>

            <!-- Output Layer -->
            <div class="component output-box">
                <div class="component-title">Output Layer</div>
                <div class="component-subtitle">(Vector → Token)</div>
            </div>

            <div class="arrow">↓</div>

            <!-- Generated Text -->
            <div class="component generated-box">
                <div class="component-title">Generated Text</div>
                <div class="component-subtitle">(Shakespearean Style)</div>
            </div>
        </div>

        <!-- Specifications -->
        <div class="specifications-container">
            <div class="spec-box">
                <div class="spec-title">Training Configuration:</div>
                <div class="spec-list">
                    <div class="spec-item">100K Shakespeare text pairs</div>
                    <div class="spec-item">Block size: 50 words, Batch size: 32</div>
                    <div class="spec-item">AdamW optimizer, Learning rate: 5e-5</div>
                    <div class="spec-item">3 epochs training</div>
                    <div class="spec-item">Cross-Entropy Loss</div>
                </div>
            </div>

            <div class="spec-box">
                <div class="spec-title">Model Specifications:</div>
                <div class="spec-list">
                    <div class="spec-item">Base Model: bert-base-uncased</div>
                    <div class="spec-item">Architecture: Transformer Encoder-Decoder</div>
                    <div class="spec-item">Hidden Size: 768, Attention Heads: 12</div>

                    <div class="spec-item">BERTScore F1: 0.8019</div>
                </div>
            </div>
        </div>
    </div>
</body>
</html>